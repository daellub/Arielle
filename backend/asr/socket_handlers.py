# backend/asr/socket_handlers.py

import asyncio
import azure.cognitiveservices.speech as speechsdk
import numpy as np

from backend.sio import sio
from backend.asr.model_manager import model_manager
from backend.utils.encryption import decrypt

# sidë³„ SpeechRecognizer ì €ì¥
recognizers = {}

# Whisper / HuggingFaceìš© ë¡œì»¬ ëª¨ë¸ ì²˜ë¦¬ ë©”ì»¤ë‹ˆì¦˜
@sio.on('start_transcribe')
async def start_transcribe(sid, data):
    print(f"[DEBUG] â–¶ start_transcribe called: sid={sid}, data={data}")
    model_id = data.get("model_id")

    if model_id not in model_manager.models:
        return await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, room=sid)
    
    model = model_manager.models[model_id]["instance"]
    if model is None:
        await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}, to=sid)
        return
    
    await sio.save_session(sid, {'model_id': model_id})

    await sio.emit('transcript', {'text': 'ğŸ™ ì „ì‚¬ ì¤€ë¹„ ì™„ë£Œ'}, to=sid)

# Azure APIìš© ëª¨ë¸ ë©”ì»¤ë‹ˆì¦˜
@sio.on('start_azure_mic')
async def start_azure_mic(sid, data):
    print(f'[SOCKET] start_azure_mic ìš”ì²­ ë°›ìŒ from {sid}')
    model_id = data.get('model_id')

    if not model_id or model_id not in model_manager.models:
        await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, to=sid)
        return

    entry = model_manager.models[model_id]
    instance = entry["instance"]
    info = entry["info"]
    framework = info.framework.lower()

    if instance is None and framework != 'azure':
        await sio.emit('transcript', {'text': 'âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}, to=sid)
        return  
    
    try:
        info.apiKey = decrypt(info.apiKey)
    except Exception as e:
        await sio.emit('transcript', {'text': f'âŒ API Key ë³µí˜¸í™” ì‹¤íŒ¨: {e}'}, to=sid)
        return
    
    await recognized_from_microphone(sid, info)

async def recognized_from_microphone(sid: str, model_info):
    apiKey = model_info.apiKey
    endpoint = model_info.endpoint
    region = model_info.region

    if not apiKey:
        raise ValueError('âŒ API Keyê°€ ì—†ìŠµë‹ˆë‹¤.')
    
    if endpoint:
        speech_config = speechsdk.SpeechConfig(subscription=apiKey, endpoint=endpoint)
    elif region:
        speech_config = speechsdk.SpeechConfig(subscription=apiKey, region=region)
    else:
        raise ValueError("âŒ ì—”ë“œí¬ì¸íŠ¸ì™€ ë¦¬ì „ì´ ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤.")
    
    speech_config.speech_recognition_language = 'ko-KR'

    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)
    speech_recognizer = speechsdk.SpeechRecognizer(
        speech_config=speech_config,
        audio_config=audio_config
    )

    loop = asyncio.get_running_loop()
    done_future = loop.create_future()

    recognizers[sid] = speech_recognizer

    def recognizing_cb(evt):
        text = evt.result.text
        if text:
            print(f'[Recognizing] {text}')
    
    def recognized_cb(evt):
        text = evt.result.text
        if text:
            print(f'[Recognized] {text}')
            asyncio.run_coroutine_threadsafe(
                sio.emit('transcript', {'text': text}, to=sid),
                loop
            )

    def session_stopped_cb(evt):
        print('[Session Stopped]')
        if not done_future.done():
            loop.call_soon_threadsafe(done_future.set_result, True)

    def canceled_cb(evt):
        print('[Canceled]', evt)
        if not done_future.done():
            loop.call_soon_threadsafe(done_future.set_result, True)
    
    speech_recognizer.recognizing.connect(recognizing_cb)
    speech_recognizer.recognized.connect(recognized_cb)
    speech_recognizer.session_stopped.connect(session_stopped_cb)
    speech_recognizer.canceled.connect(canceled_cb)

    print("ğŸ™ Azure ë§ˆì´í¬ ì¸ì‹ ì‹œì‘")
    await sio.emit('transcript', {'text': 'ğŸ™ Azure STT ìŠ¤íŠ¸ë¦¬ë° ì¤€ë¹„ ì™„ë£Œ'}, to=sid)

    speech_recognizer.start_continuous_recognition()
    await done_future
    speech_recognizer.stop_continuous_recognition()
    print("ğŸ™ Azure ë§ˆì´í¬ ì¸ì‹ ì¢…ë£Œ")

    if sid in recognizers:
        del recognizers[sid]

@sio.on('audio_chunk')
async def audio_chunk(sid, data):
    session = await sio.get_session(sid)
    model_id = session.get("model_id")

    if not model_id or model_id not in model_manager.models:
        await sio.emit('transcript', {'text': 'âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ì…ë‹ˆë‹¤.'},  to=sid)
        return
    
    try:
        audio_np = np.array(data, dtype=np.float32)

        texts = model_manager.infer(model_id, audio_np, language="<|ko|>")
        print("[DEBUG] ì „ì‚¬ ê²°ê³¼: ", texts)
        if texts:
            await sio.emit('transcript', {'text': texts[0]}, to=sid)
    except Exception as e:
        print(f"[ERROR] audio_chunk ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        await sio.emit('transcript', {'text': 'âŒ ì „ì‚¬ ì‹¤íŒ¨'}, to=sid)

@sio.on('stop_transcribe')
async def stop_transcribe(sid):
    print(f'[SOCKET] stop_transcribe ìš”ì²­ ë°›ìŒ from {sid}')

# Azure ì „ì‚¬ ì¤‘ë‹¨
@sio.on('stop_azure_mic')
async def stop_azure_mic(sid, data):
    print(f'[SOCKET] stop_azure_mic ìš”ì²­ ë°›ìŒ from {sid}')
    recognizer = recognizers.get(sid)
    if recognizer:
        recognizer.stop_continuous_recognition()
        print(f"[INFO] SpeechRecognizer ì¤‘ì§€ ì™„ë£Œ for {sid}")
        del recognizers[sid]
    else:
        print(f"[WARN] stop_azure_mic: recognizer ì—†ìŒ for {sid}")